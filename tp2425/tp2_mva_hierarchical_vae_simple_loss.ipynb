{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/generativemodelingmva/generativemodelingmva.github.io/blob/main/tp2_mva_hierarchical_vae_simple_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2477ce57",
      "metadata": {
        "id": "2477ce57"
      },
      "source": [
        "# Hierarchical VAE\n",
        "\n",
        "\n",
        "This practice session allows to train a top-down hierarchical VAE.\n",
        "\n",
        "This is a toy implementation of the NVAE model with a simple MSE reconstruction loss and only three stages.\n",
        "\n",
        "![alternatvie text](https://raw.githubusercontent.com/generativemodelingmva/generativemodelingmva.github.io/main/tp2324/toynvae_framework_full.png)\n",
        "\n",
        "\n",
        "Sources:\n",
        "* Toy implementation from: https://github.com/GlassyWing/nvae\n",
        "* NVAE official implementation: https://github.com/NVlabs/NVAE\n",
        "* NVAE paper: \"NVAE: A Deep Hierarchical Variational Autoencoder\", Arash Vahdat and Jan Kautz (NeurIPS 2020 Spotlight Paper) https://arxiv.org/abs/2007.03898\n",
        "* CelebA validation set (used for training): https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f74528",
      "metadata": {
        "id": "05f74528"
      },
      "source": [
        "# Download files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58483fde",
      "metadata": {
        "id": "58483fde"
      },
      "outputs": [],
      "source": [
        "# do just once:\n",
        "!wget -nc -O celeba64png_val.zip 'https://www.dropbox.com/scl/fi/3d2le2wlu61nzkbxymfm3/celeba64png_val.zip?rlkey=ckesud01kwb8tualsd3s8zg5d'\n",
        "!unzip -nq celeba64png_val.zip\n",
        "!ls val | wc -l # there should be 19867 files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba87051",
      "metadata": {
        "id": "eba87051"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d66fc5c",
      "metadata": {
        "id": "4d66fc5c"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "class PngImageFolderDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_dir):\n",
        "        self.img_paths = sorted(glob(os.path.join(image_dir, \"*.png\")))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torchvision.transforms.ToTensor()(Image.open(self.img_paths[idx]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "train_ds = PngImageFolderDataset('val')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb808489",
      "metadata": {
        "id": "bb808489"
      },
      "source": [
        "# Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1982c0e",
      "metadata": {
        "id": "e1982c0e"
      },
      "source": [
        "## Losses for training\n",
        "\n",
        "$\\newcommand{\\bx}{\\mathbf{x}} % bold x$\n",
        "$\\newcommand{\\bz}{\\mathbf{z}} % bold z$\n",
        "$\\newcommand{\\DKL}{D_{\\mathrm{KL}}}$\n",
        "$\\newcommand{\\E}{\\mathbb{E}}$\n",
        "$\\newcommand{\\bmu}{{\\mathbf{\\mu}}} % bold mu$\n",
        "$\\newcommand{\\bsigma}{{\\mathbf{\\sigma}}} % bold Sigma$\n",
        "$\\newcommand{\\diag}{\\operatorname{diag}}$\n",
        "The training loss for the hierarchical VAE is\n",
        "$$\n",
        "    \\mathcal{L}_{\\text{VAE}}(\\bx) :\n",
        "    = \\E_{q(\\bz|\\bx)}\\left[ \\log p(\\bx|\\bz) \\right] - \\DKL(q(\\bz_1|\\bx)|p(\\bz_1)) - \\sum_{l=2}^L \\E_{q(\\bz_{<l}|\\bx)} \\left[ \\DKL(q(\\bz_l|\\bx, \\bz_{<l})|p(\\bz_l|\\bz_{<l})) \\right]\n",
        "$$\n",
        "Each expectation term will be approximated by a one sample approximation using the parameterization trick.\n",
        "For this practice session, we use normal distribution for all terms.\n",
        "\n",
        "Following the NVAE paper, we suppose that\n",
        "$\n",
        "p(\\bz_l|\\bz_{<l}) = \\mathcal{N} \\left(\\bmu(\\bz_{<l}), \\diag{\\bsigma^2}(\\bz_{<l}) \\right)\n",
        "$\n",
        "and\n",
        "$$\n",
        "q(\\bz_l|\\bz_{<l}, \\bx) = \\mathcal{N} \\left(\\bmu(\\bz_{<l}) + \\Delta \\bmu(\\bz_{<l}, \\bx), \\diag\\left(\\bsigma^2(\\bz_{<l}) \\cdot \\Delta \\bsigma^2(\\bz_{<l}, \\bx)\\right) \\right).\n",
        "$$\n",
        "\n",
        "### Exercise 1:\n",
        "Let us recall the general expression\n",
        "$$\n",
        "D_{KL}(\n",
        "\\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1)\n",
        "\\parallel\n",
        "\\mathcal{N}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_2)\n",
        ") =\n",
        "\\frac{1}{2} \\left[ \\log \\frac{|\\boldsymbol{\\Sigma}_2|}{|\\boldsymbol{\\Sigma}_1|} - d + \\text{tr}(\\boldsymbol{\\Sigma}_2^{-1} \\boldsymbol{\\Sigma}_1) + (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_2^{-1} (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1) \\right].\n",
        "$$\n",
        "\n",
        "\n",
        "1. Compute the expression of\n",
        "$\\DKL(q(\\bz_l|\\bx, \\bz_{<l})|p(\\bz_l|\\bz_{<l}))$\n",
        "in function of $\\Delta \\bmu(\\bz_{<l}, \\bx)$, $\\Delta \\bsigma^2(\\bz_{<l}, \\bx)$ and ${\\bsigma^2}(\\bz_{<l})$.\n",
        "1. Implement a funcion **kl_delta(delta_mu, delta_log_var, mu, log_var)** that computes $\\DKL(q(\\bz_l|\\bx, \\bz_{<l})|p(\\bz_l|\\bz_{<l}))$ in function of its parameters (note that the decoder outputs the logarithm of the diagonal variances).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686820d6",
      "metadata": {
        "id": "686820d6"
      },
      "outputs": [],
      "source": [
        "# utils functions for training and sampling:\n",
        "\n",
        "def kl(mu, log_var):\n",
        "    \"\"\"\n",
        "    kl loss with standard norm distribute\n",
        "    :param mu:\n",
        "    :param log_var:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    loss = -0.5 * torch.sum(1 + log_var - mu ** 2 - torch.exp(log_var), dim=[1, 2, 3])\n",
        "    return torch.mean(loss, dim=0)\n",
        "\n",
        "\n",
        "def kl_delta(delta_mu, delta_log_var, mu, log_var):\n",
        "    # TODO\n",
        "\n",
        "\n",
        "def reparameterize(mu, std):\n",
        "    z = torch.randn_like(mu) * std + mu\n",
        "    return z\n",
        "\n",
        "\n",
        "from torch.nn.utils import spectral_norm\n",
        "def add_sn(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        return spectral_norm(m)\n",
        "    else:\n",
        "        return m\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98354948",
      "metadata": {
        "id": "98354948"
      },
      "source": [
        "### Common layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18bbd515",
      "metadata": {
        "id": "18bbd515"
      },
      "outputs": [],
      "source": [
        "#from nvae.common import Swish, DecoderResidualBlock, ResidualBlock\n",
        "\n",
        "class Swish(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class DecoderResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, n_group):\n",
        "        super().__init__()\n",
        "\n",
        "        self._seq = nn.Sequential(\n",
        "            nn.Conv2d(dim, n_group * dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(n_group * dim), Swish(),\n",
        "            nn.Conv2d(n_group * dim, n_group * dim, kernel_size=5, padding=2, groups=n_group),\n",
        "            nn.BatchNorm2d(n_group * dim), Swish(),\n",
        "            nn.Conv2d(n_group * dim, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            SELayer(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self._seq(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self._seq = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(dim, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim), Swish(),\n",
        "            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
        "            SELayer(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self._seq(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d9c069",
      "metadata": {
        "id": "03d9c069"
      },
      "source": [
        "### Encoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82bf8a78",
      "metadata": {
        "id": "82bf8a78"
      },
      "outputs": [],
      "source": [
        "# Encoder:\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self._seq = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(out_channel, out_channel // 2, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channel // 2), Swish(),\n",
        "            nn.Conv2d(out_channel // 2, out_channel, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channel), Swish()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._seq(x)\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        modules = []\n",
        "        for i in range(len(channels) - 1):\n",
        "            modules.append(ConvBlock(channels[i], channels[i + 1]))\n",
        "\n",
        "        self.modules_list = nn.ModuleList(modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for module in self.modules_list:\n",
        "            x = module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(dim, dim, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(dim, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim), Swish(),\n",
        "            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
        "            SELayer(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self.seq(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            EncoderBlock([3, z_dim // 16, z_dim // 8]),  # (16, 16)\n",
        "            EncoderBlock([z_dim // 8, z_dim // 4, z_dim // 2]),  # (4, 4)\n",
        "            EncoderBlock([z_dim // 2, z_dim]),  # (2, 2)\n",
        "        ])\n",
        "\n",
        "        self.encoder_residual_blocks = nn.ModuleList([\n",
        "            EncoderResidualBlock(z_dim // 8),\n",
        "            EncoderResidualBlock(z_dim // 2),\n",
        "            EncoderResidualBlock(z_dim),\n",
        "        ])\n",
        "\n",
        "        self.condition_x = nn.Sequential(\n",
        "            Swish(),\n",
        "            nn.Conv2d(z_dim, z_dim * 2, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        xs = []\n",
        "        for e, r in zip(self.encoder_blocks, self.encoder_residual_blocks):\n",
        "            x = r(e(x))\n",
        "            xs.append(x)\n",
        "\n",
        "        mu, log_var = self.condition_x(x).chunk(2, dim=1)\n",
        "\n",
        "        return mu, log_var, xs[:-1][::-1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb85966",
      "metadata": {
        "id": "edb85966"
      },
      "source": [
        "### Decoder architecture with shared top-down encoder/decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9875c6",
      "metadata": {
        "id": "ae9875c6"
      },
      "outputs": [],
      "source": [
        "# Decoder:\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self._seq = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(in_channel,\n",
        "                               out_channel,\n",
        "                               kernel_size=3,\n",
        "                               stride=2,\n",
        "                               padding=1,\n",
        "                               output_padding=1),\n",
        "            # nn.UpsamplingBilinear2d(scale_factor=2),\n",
        "            # nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channel), Swish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._seq(x)\n",
        "\n",
        "\n",
        "class DecoderResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, n_group):\n",
        "        super().__init__()\n",
        "\n",
        "        self._seq = nn.Sequential(\n",
        "            nn.Conv2d(dim, n_group * dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(n_group * dim), Swish(),\n",
        "            nn.Conv2d(n_group * dim, n_group * dim, kernel_size=5, padding=2, groups=n_group),\n",
        "            nn.BatchNorm2d(n_group * dim), Swish(),\n",
        "            nn.Conv2d(n_group * dim, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            SELayer(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self._seq(x)\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        modules = []\n",
        "        for i in range(len(channels) - 1):\n",
        "            modules.append(UpsampleBlock(channels[i], channels[i + 1]))\n",
        "        self.module_list = nn.ModuleList(modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for module in self.module_list:\n",
        "            x = module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input channels = z_channels * 2 = x_channels + z_channels\n",
        "        # Output channels = z_channels\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock([z_dim * 2, z_dim // 2]),  # 2x upsample\n",
        "            DecoderBlock([z_dim, z_dim // 4, z_dim // 8]),  # 4x upsample\n",
        "            DecoderBlock([z_dim // 4, z_dim // 16, z_dim // 32])  # 4x uplsampe\n",
        "        ])\n",
        "        self.decoder_residual_blocks = nn.ModuleList([\n",
        "            DecoderResidualBlock(z_dim // 2, n_group=4),\n",
        "            DecoderResidualBlock(z_dim // 8, n_group=2),\n",
        "            DecoderResidualBlock(z_dim // 32, n_group=1)\n",
        "        ])\n",
        "\n",
        "        # p(z_l | z_(l-1))\n",
        "        self.condition_z = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(z_dim // 2),\n",
        "                Swish(),\n",
        "                nn.Conv2d(z_dim // 2, z_dim, kernel_size=1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(z_dim // 8),\n",
        "                Swish(),\n",
        "                nn.Conv2d(z_dim // 8, z_dim // 4, kernel_size=1)\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # p(z_l | x, z_(l-1))\n",
        "        self.condition_xz = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(z_dim),\n",
        "                nn.Conv2d(z_dim, z_dim // 2, kernel_size=1),\n",
        "                Swish(),\n",
        "                nn.Conv2d(z_dim // 2, z_dim, kernel_size=1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(z_dim // 4),\n",
        "                nn.Conv2d(z_dim // 4, z_dim // 8, kernel_size=1),\n",
        "                Swish(),\n",
        "                nn.Conv2d(z_dim // 8, z_dim // 4, kernel_size=1)\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        self.recon = nn.Sequential(\n",
        "            ResidualBlock(z_dim // 32),\n",
        "            nn.Conv2d(z_dim // 32, 3, kernel_size=1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, z, xs=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param z: shape. = (B, z_dim, map_h, map_w)\n",
        "        if xs=None: sample mode; otherwise xs is list of intermediate encoder features\n",
        "        \"\"\"\n",
        "\n",
        "        B, D, map_h, map_w = z.shape\n",
        "\n",
        "        # The init h (hidden state), can be replace with learned param, but it didn't work much\n",
        "        decoder_out = torch.zeros(B, D, map_h, map_w, device=z.device, dtype=z.dtype)\n",
        "\n",
        "        kl_losses = []\n",
        "\n",
        "        for i in range(len(self.decoder_residual_blocks)):\n",
        "\n",
        "            z_sample = torch.cat([decoder_out, z], dim=1)\n",
        "            decoder_out = self.decoder_residual_blocks[i](self.decoder_blocks[i](z_sample))\n",
        "\n",
        "            if i == len(self.decoder_residual_blocks) - 1: # stop if last block\n",
        "                break\n",
        "\n",
        "            mu, log_var = self.condition_z[i](decoder_out).chunk(2, dim=1) # parameter for sampling next z\n",
        "\n",
        "            if xs is not None:\n",
        "                delta_mu, delta_log_var = self.condition_xz[i](\n",
        "                                torch.cat([xs[i], decoder_out], dim=1)).chunk(2, dim=1)\n",
        "                kl_losses.append(kl_delta(delta_mu, delta_log_var, mu, log_var))\n",
        "                mu = mu + delta_mu\n",
        "                log_var = log_var + delta_log_var\n",
        "\n",
        "            z = reparameterize(mu, torch.exp(0.5 * log_var))\n",
        "\n",
        "        x_hat = torch.sigmoid(self.recon(decoder_out))\n",
        "\n",
        "        return x_hat, kl_losses\n",
        "\n",
        "\n",
        "    def sample(self, n_samples=32, fix_level=-1):\n",
        "      # TODO EXERCISE 3\n",
        "      \"\"\"\n",
        "      n_samples: number of images to sample\n",
        "      fix_level=-1: if fix_level = 0, (resp. 1) all samples have the same z0 (resp. z0 and z1)\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e80077a",
      "metadata": {
        "id": "8e80077a"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim, img_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(z_dim)\n",
        "        self.decoder = Decoder(z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "\n",
        "        :param x: Tensor. shape = (B, C, H, W)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        mu, log_var, xs = self.encoder(x)\n",
        "\n",
        "        # (B, D_Z)\n",
        "        z = reparameterize(mu, torch.exp(0.5 * log_var)) # sampling top latent variable\n",
        "\n",
        "        decoder_output, kl_losses = self.decoder(z, xs)\n",
        "\n",
        "        kl_losses = [kl(mu, log_var)]+kl_losses\n",
        "\n",
        "        recon_loss = nn.MSELoss(reduction='sum')(decoder_output, x)/decoder_output.shape[0]\n",
        "\n",
        "        return decoder_output, recon_loss, kl_losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization functions"
      ],
      "metadata": {
        "id": "duat51BAd7gP"
      },
      "id": "duat51BAd7gP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b736f686",
      "metadata": {
        "id": "b736f686"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "def imshow(img):\n",
        "    #img = img*0.5 + 0.5     # unnormalize\n",
        "    pil_img = torchvision.transforms.functional.to_pil_image(img)\n",
        "    display(pil_img)\n",
        "    #print(\"Image size (h x w): \",  pil_img.height, \"x\", pil_img.width)\n",
        "    return(pil_img)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "model = NVAE(z_dim=512, img_dim=(64, 64)).to(device)\n",
        "\n",
        "def show_decoder_output(z=None):\n",
        "  # provide random latent code as option to see evolution\n",
        "  with torch.no_grad():\n",
        "    if z==None:\n",
        "      z = torch.randn((batch_size,512,2,2)).to(device)\n",
        "      # We use full batch size and then select first 32 images\n",
        "    genimages = model.decoder(z)[0]\n",
        "    pil_img = imshow(torchvision.utils.make_grid(genimages[:32,:,:,:].to('cpu'),nrow=8))\n",
        "  return(pil_img)\n",
        "\n",
        "show_decoder_output();\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75fc56ea",
      "metadata": {
        "id": "75fc56ea"
      },
      "source": [
        "# Training\n",
        "\n",
        "### Exercise 2:\n",
        "1. Display an image of 4x8 portraits from the training dataset.\n",
        "1. Read the model architecture and train for 5 epochs.\n",
        "1. How do you explain the difference of images generated with and without model.eval()?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Display an image of 4x8 portraits from the training dataset.\n"
      ],
      "metadata": {
        "id": "8bE9SXI9E8r3"
      },
      "id": "8bE9SXI9E8r3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f16c5d",
      "metadata": {
        "scrolled": false,
        "id": "e8f16c5d"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 128\n",
        "n_cpu = 2\n",
        "\n",
        "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=n_cpu)\n",
        "\n",
        "model = NVAE(z_dim=512, img_dim=(64, 64)).to(device)\n",
        "# apply Spectral Normalization\n",
        "model.apply(add_sn)\n",
        "\n",
        "\n",
        "zshow = torch.randn((batch_size,512,2,2)).to(device)\n",
        "\n",
        "\n",
        "# folder for checkpoints and visualization:\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "checkpoints_dir = dt_string+\"_checkpoints\"\n",
        "os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "outputs_dir = dt_string+\"_outputs\"\n",
        "os.makedirs(outputs_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-4)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    for i, image in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        image = image.to(device)\n",
        "        image_recon, recon_loss, kl_losses = model(image)\n",
        "\n",
        "        kl_f = 1.\n",
        "        loss = recon_loss + kl_f*sum(kl_losses)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            log_str = \"\\r---- [Epoch %d/%d, Step %d/%d] loss: %.6f----\" % (\n",
        "            epoch, epochs, i, len(train_dataloader), loss.item())\n",
        "            print(log_str)\n",
        "            with torch.no_grad():\n",
        "                pil_img = show_decoder_output(zshow)\n",
        "                imgpath = os.path.join(outputs_dir, \"nvae_simple_loss_epoch_\"+str(epoch).zfill(3)+\"_step_\"+str(i).zfill(4)+\".png\")\n",
        "                pil_img.save(imgpath)\n",
        "                model.eval()\n",
        "                pil_img = show_decoder_output(zshow)\n",
        "                imgpath = os.path.join(outputs_dir, \"nvae_simple_loss_epoch_\"+str(epoch).zfill(3)+\"_step_\"+str(i).zfill(4)+\"_eval.png\")\n",
        "                pil_img.save(imgpath)\n",
        "                model.train()\n",
        "\n",
        "    # end epoch: save checkpoint:\n",
        "    scheduler.step()\n",
        "    if epoch%5==4:\n",
        "      checkpoint_path = os.path.join(checkpoints_dir, \"nvae_simple_loss_epoch_\"+str(epoch).zfill(3)+\".pth\")\n",
        "      torch.save(model.state_dict(), checkpoint_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d022f1",
      "metadata": {
        "id": "d4d022f1"
      },
      "source": [
        "### Load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7df4ea",
      "metadata": {
        "id": "7c7df4ea"
      },
      "outputs": [],
      "source": [
        "# Load pretrained model:\n",
        "!wget -nc -O nvae_simple_loss_epoch_199.pth 'https://www.dropbox.com/scl/fi/0rsjcx78w338nj4ie6lun/nvae_simple_loss_epoch_199.pth?rlkey=8ok3htl9gp3ywmpqu02xdfo87'\n",
        "checkpoint_path = 'nvae_simple_loss_epoch_199.pth'\n",
        "model = NVAE(z_dim=512, img_dim=(64, 64)).to(device)\n",
        "model.apply(add_sn)\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n",
        "model.eval()\n",
        "\n",
        "zshow = torch.randn((batch_size,512,2,2)).to(device)\n",
        "show_decoder_output(zshow);\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae8a7d2",
      "metadata": {
        "id": "9ae8a7d2"
      },
      "source": [
        "### Exercise 3:\n",
        "1. Complete the function def sample(self, n_samples=32, fix_level=-1) of the encoder class so that it samples images with common realizations up to fix_level (begin by implementing and testing independent sampling).\n",
        "1. Test this function and verify the hierarchical VAE encodes the images hierarchically.\n",
        "1. (Bonus question) Sample 20k images and compute FID against the celeba test set (available here https://www.dropbox.com/scl/fi/in8hqobto2p2k2baiwi5x/celeba64png_test.zip?rlkey=jmisq9swucjwjyv69ftwwks06)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1860072e",
      "metadata": {
        "scrolled": true,
        "id": "1860072e"
      },
      "outputs": [],
      "source": [
        "# TODO sampling\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch21",
      "language": "python",
      "name": "pytorch21"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}