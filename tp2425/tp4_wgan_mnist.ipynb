{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/generativemodelingmva/generativemodelingmva.github.io/blob/main/tp2425/tp4_wgan_mnist.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzo652OIfgkQ"
   },
   "source": [
    "# Convolutional Generative Network for MNIST digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS78gxFngGi-"
   },
   "source": [
    "This practical session is based on the [DCGAN Pytorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html).\n",
    "\n",
    "It was adapted by\n",
    "* Lucía Bouza\n",
    "* Bruno Galerne\n",
    "* Arthur Leclaire\n",
    "\n",
    "You should complete the code regions marked with ###...###."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds7HbRCmgCwu"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# default directory:\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks\n",
    "# we advise to create a specific directory on your Google drive:\n",
    "%cd /content/drive/MyDrive/genmod2425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LHhzJBLSNOh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\", device)\n",
    "!nvidia-smi\n",
    "\n",
    "# Displaying function\n",
    "def imshow(img,size=None):\n",
    "    img = img*0.5 + 0.5     # unnormalize\n",
    "    if size is not None:\n",
    "      img = transforms.Resize(size=size, interpolation=transforms.InterpolationMode.NEAREST, antialias=True)(img)\n",
    "    pil_img = torchvision.transforms.functional.to_pil_image(img)\n",
    "    display(pil_img)\n",
    "    # print(\"Image size (h x w): \",  pil_img.height, \"x\", pil_img.width)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x11o4K94h6CY"
   },
   "source": [
    "## Download MNIST dataset\n",
    "\n",
    "Note that we normalize the images between -1 and 1 because during sampling, we have to limit the input space and scaling between -1 and 1 makes it easier to implement it. We discard the last batch so that all batches have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2Scj6lGiByO"
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "train_set = MNIST(os.getcwd(), train=True, transform=transform, download=True)\n",
    "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Draw a batch of real images with the train_loader and display them. Use `next` and `iter` to get a batch from `train_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real,_ = ### ... ###\n",
    "print(real.shape)\n",
    "    \n",
    "pil_img = imshow(torchvision.utils.make_grid(real.to('cpu'),nrow=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5fs7sxuQyY8"
   },
   "source": [
    "## Generator and Discriminator Models\n",
    "\n",
    "The architecture guidelines for stable DCGANs mentioned in the [paper](https://arxiv.org/pdf/1511.06434.pdf) are:\n",
    "- Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). See this [github site](https://github.com/vdumoulin/conv_arithmetic) to view animations of all kind of convolutions. \n",
    "- Use batchnorm in both the generator and the discriminator.\n",
    "- Remove fully connected hidden layers for deeper architectures.\n",
    "- Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "- Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n",
    "**The generator** maps $z$ (a latent space vector sampled from a standard normal distribution) to data-space (in our case images the MNIST images having size 1x28x28).\n",
    "\n",
    "In practice, this is accomplished through a series of Conv2DTranspose (Upsampling) layers each paired with a 2D batch norm layer and a Relu activation. We upsample the image three times so as to reach the desired image size. The output of the generator is fed through a tanh function to return it to the input data range of [-1,1].\n",
    "\n",
    "**The discriminator** outputs the probability that the input (an image) came from real MNIST images rather than the generator. \n",
    "\n",
    "In practice, this is accomplished through a series of Conv2d, 2D batch norm, and LeakyReLU layers, and outputs the final probability through a Sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbyuGWJQREbg"
   },
   "outputs": [],
   "source": [
    "# Size  of generator input\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator and discriminator\n",
    "ngf, ndf = 64, 64\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(a\n",
    "            nn.ConvTranspose2d(in_channels = nz, out_channels = ngf * 8, kernel_size = 4, stride = 1, padding = 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(in_channels = ngf * 8, out_channels = ngf * 4, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(in_channels = ngf * 4, out_channels = ngf * 2, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(in_channels = ngf * 2, out_channels = ngf, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(in_channels = ngf, out_channels = 1, kernel_size=1, stride=1, padding=2, bias=False),\n",
    "            nn.Tanh()\n",
    "            # output size. 1 x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is 1 x 28 x 28\n",
    "            nn.Conv2d(in_channels = 1, out_channels = ndf, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 15 x 15\n",
    "            nn.Conv2d(in_channels = ndf, out_channels= ndf * 2, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 8 x 8\n",
    "            nn.Conv2d(in_channels = ndf * 2, out_channels = ndf * 4, kernel_size = 4, stride = 2, padding = 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 5 x 5\n",
    "            nn.Conv2d(in_channels = ndf * 4, out_channels = 1, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1).squeeze(1)\n",
    "    \n",
    "# check sizes:\n",
    "import torchsummary\n",
    "\n",
    "# Create some generator and discriminator\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "torchsummary.summary(G, input_size=(nz,1,1))\n",
    "torchsummary.summary(D, input_size=(1,28,28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Samples of the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oLYU5_llQ3O"
   },
   "outputs": [],
   "source": [
    "# function to display samples of the generator\n",
    "def show(G,z=None,batch_size=128,nz=100):\n",
    "  # provide random latent code as option to see evolution\n",
    "  with torch.no_grad():\n",
    "    if z==None:\n",
    "      z = torch.randn(batch_size,nz,1,1).to(device)\n",
    "    genimages = G(z)\n",
    "    pil_img = imshow(torchvision.utils.make_grid(genimages.to('cpu'),nrow=16))\n",
    "    return(pil_img)\n",
    "\n",
    "# Load a pre-learned generator to see what you will get at the end of the practical session!:\n",
    "G = Generator().to(device)\n",
    "G.load_state_dict(torch.hub.load_state_dict_from_url('https://perso.telecom-paristech.fr/aleclaire/mva/tp/wgan_epoch100.pt', progress=False))\n",
    "\n",
    "# Display samples\n",
    "### ... ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibgOtzGg6Xx4"
   },
   "source": [
    "## Weight initialization\n",
    "\n",
    "The DCGAN [paper](https://arxiv.org/pdf/1511.06434.pdf) mentions that all model weights shall be randomly initialized from a Normal distribution with $\\mu=0$ and $\\sigma=0.02$. We implement `weights_init` function to reinitialize the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZDe_VPeRqTg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Create the generator and discriminator\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.02.\n",
    "G.apply(weights_init);\n",
    "D.apply(weights_init);\n",
    "\n",
    "show(G);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GswgbnEDwviP"
   },
   "source": [
    "# Exercise 1: DCGAN Training with WGAN-GP loss\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Implement functions to estimate the Lipschitz constant and Gradient Penalty of the discriminator, and test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lipconstant(D,x,y):\n",
    "    ### ... ###\n",
    "\n",
    "def gradient_penalty(D,x,y):\n",
    "    ### ... ###\n",
    "\n",
    "# draw a batch of real samples\n",
    "y = ### ... ###\n",
    "# draw a batch of fake samples\n",
    "x =  ### ... ###\n",
    "\n",
    "print(lipconstant(D,x,y))\n",
    "print(gradient_penalty(D,x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Implement WGAN-GP training for MNIST by completing the code in the following cell.\n",
    "We recall the pseudo-code:\n",
    "\n",
    "> For each batch of images $x_{\\text{real}}$:\n",
    ">\n",
    "> **1) Train discriminator:**\n",
    "> > Generate $z$ a tensor of size $b\\times nz\\times 1\\times 1$ of idd Gaussian variables  \n",
    "> > Generate  $x_{\\text{fake}} = \\mathtt{G}(z)$ a set $b$ fake images  \n",
    "> > Compute the discriminator loss <br/>\n",
    "> > Compute the gradient and do an optimizer step for the disciminator parameters  \n",
    ">\n",
    "> **2) Train the generator:**\n",
    "> > Generate $z$ a new tensor of size $b\\times nz\\times 1\\times 1$ of idd Gaussian variables  \n",
    "> > Compute the generator loss <br/>\n",
    "> > Compute the gradient and do an optimizer step for the generator parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)  # initialize random seed for reproducibility\n",
    "\n",
    "num_epochs = 5\n",
    "log_every = 100\n",
    "gpw = 0.1\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G.apply(weights_init);\n",
    "D.apply(weights_init);\n",
    "optimD = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimG = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "zviz = torch.randn(batch_size,nz,1,1).to(device)\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the train_loader\n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # Draw Batches of real and fake images\n",
    "        \n",
    "        ### ... ###\n",
    "\n",
    "        ############################\n",
    "        # Update D network\n",
    "\n",
    "        ### ... ###\n",
    "            \n",
    "        ############################\n",
    "        # Update G network\n",
    "\n",
    "        ### ... ###\n",
    "\n",
    "        ############################\n",
    "        # Display training stats and visualize\n",
    "        if i % log_every == 0:\n",
    "            print('[%d/%d][%d/%d][%.4f s]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tLip(D): %.4f' \n",
    "                  % (epoch+1, num_epochs, i, len(train_loader), time.time()-t0, Dloss.item(), Gloss.item(),lipconstant(D,real,faked)))\n",
    "            show(G,zviz)\n",
    "\n",
    "print('Total learning time = ',time.time()-t0)\n",
    "            \n",
    "# Save final generator in a variable for later use\n",
    "wgan = Generator()\n",
    "wgan.load_state_dict(G.state_dict())\n",
    "\n",
    "# Save final generator in a file\n",
    "torch.save(G.state_dict(), 'wgan.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Let's play with the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXbp5DXM__0K"
   },
   "source": [
    "## Interpolation in latent space:\n",
    "\n",
    "**QUESTION:**\n",
    "Generate 2 sets of 10 latent variable $z_0$ and $z_1$ and display the generated images by the latent variables:\n",
    "$$\n",
    "z_\\alpha = (1-\\alpha) z_0 + \\alpha z_1\n",
    "$$\n",
    "for $\\alpha$ varying between $0$ and $1$.\n",
    "\n",
    "Display all the images in a grid of height 10 and width 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmGx53H7uvsa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Uncomment the following lines if you want to load a pretrained generator:\n",
    "# G = Generator().to(device)\n",
    "# G.load_state_dict(torch.hub.load_state_dict_from_url('https://perso.telecom-paristech.fr/aleclaire/mva/tp/wgan_epoch100.pt', progress=False))\n",
    "\n",
    "\n",
    "G.eval();  # Turn generator in evaluation mode to fix BatchNorm layers\n",
    "\n",
    "minib = 10    # number of latent variables\n",
    "nk = 30       # number of alpha values\n",
    "\n",
    "### ... ###\n",
    "\n",
    "pil_img = imshow(torchvision.utils.make_grid(genimages.to('cpu'),nrow=nk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor in the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the database\n",
    "train_loader_all = data.DataLoader(train_set, batch_size=60000, shuffle=False, num_workers=2, drop_last=True)\n",
    "y,labels = next(iter(train_loader_all))\n",
    "\n",
    "# For fun, display parts of the database corresponding to different figures\n",
    "realzeros = y[labels==0]\n",
    "realones = y[labels==1]\n",
    "realtwos = y[labels==2]\n",
    "realthrees = y[labels==3]\n",
    "imshow(torchvision.utils.make_grid(realzeros[0:128,:,:,:].to('cpu'),nrow=16));\n",
    "imshow(torchvision.utils.make_grid(realones[0:128,:,:,:].to('cpu'),nrow=16));\n",
    "imshow(torchvision.utils.make_grid(realtwos[0:128,:,:,:].to('cpu'),nrow=16));\n",
    "imshow(torchvision.utils.make_grid(realthrees[0:128,:,:,:].to('cpu'),nrow=16));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** For several samples of the generative model, compute the nearest neighbors in the whole dataset.\n",
    "\n",
    "Display the samples and their nearest neighbor side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x =      # minib samples of the generative model\n",
    "\n",
    "### ... ###\n",
    "\n",
    "xnn =    # nearest neighbors in the dataset\n",
    "\n",
    "im = torch.zeros((minib*2,1,28,28)).to(device)\n",
    "im[0:minib,:,:,:] = x\n",
    "im[minib:2*minib,:,:,:] = xnn\n",
    "\n",
    "imshow(torchvision.utils.make_grid(im.to('cpu'),nrow=10),size=200)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
