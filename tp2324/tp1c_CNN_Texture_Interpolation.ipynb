{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3aqjgrs4f6U"
   },
   "source": [
    "# Texture interpolation with CNNs in PyTorch\n",
    "\n",
    "<br/><br/>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/storimaging/Notebooks/blob/main/ImageGeneration/CNN_Texture_Interpolation.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsZ7qPBk4tO0"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This practical session explains how to implement the Texture Interpolation between arbitrary textures based on the algorithm described on [J. Vacher, A. Davila, A. Kohn, and R. Coen-Cagli,Texture interpolation for probingvisual perception, Advances in Neural Information Processing Systems, (2020)](https://arxiv.org/pdf/2006.03698.pdf).\n",
    "\n",
    "**Texture interpolation:**\n",
    "\n",
    "Texture interpolation or mixing consists of generating new textures by mixing different examples of textures.\n",
    "\n",
    "**References:**\n",
    "\n",
    "This practical session is based on several resources:\n",
    "\n",
    "*   Original code: https://github.com/JonathanVacher/texture-interpolation\n",
    "*   Functions to manage images: https://github.com/trsvchn/deep-textures\n",
    "*   Texture Synthesis Notebook: https://github.com/storimaging/Notebooks/blob/master/CNN_Texture_Synthesis.ipynb\n",
    "*   Tutorial used for some explanation: https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "\n",
    "**Authors:**\n",
    "* Lucía Bouza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3u0-27H_xCN"
   },
   "source": [
    "Let us recall the algorithm proposed by Gatys et al. and implemented in the Texture synthesis notebook.\n",
    "\n",
    "We will now replace the loss function with the one suggested in the aforementioned [paper](https://arxiv.org/pdf/2006.03698.pdf). *Wasserstein* loss $L_W$ aims to minimize with respect to $x$ the Wasserstein distance between the input and the target feature vectors :\n",
    "\n",
    "$$\n",
    "L_W(x, u) = \\sum_{\\text{for selected layers } \\ell} \\left\\| \\operatorname{mean}(x_\\ell) - \\operatorname{mean}(u_\\ell) \\right\\|^2 + B(\\operatorname{Cov}(x_\\ell), \\operatorname{Cov}(u_\\ell))^2\n",
    "$$\n",
    "where:\n",
    "- $ B(X, Y)^2 = \\operatorname{Tr} (X + Y -2(X^{1/2}YX^{1/2})^{1/2})$ is the Bures metric between symmetric matrices $X$ and $Y$\n",
    "- $x_\\ell$ denotes the feature vector of layer $\\ell$ for image $x$ which is our optimzation variable,\n",
    "- $u_\\ell$ denotes the feature vector of layer $\\ell$ for input image $u$.\n",
    "\n",
    "As soon as the covariance matrices commute, we simply have:\n",
    "$$\n",
    "B(X, Y)^2 = \\left\\| X^{1/2} - Y^{1/2} \\right\\|^2_F.\n",
    "$$\n",
    "In what follows we will always assume that the covariance matrices commute.\n",
    "\n",
    "We can have $K$ target textures to interpolate. The interpolation $x^\\star$ can be found with the following expression:\n",
    "\n",
    "$$\n",
    "x^\\star = \\operatorname{argmin}_x  \\sum^{K}_{k=1} \\lambda_k L_W(x, u_k)\n",
    "$$\n",
    "where the wieghts $\\lambda_k >0$ are such that $ \\sum^{K}_{k=1} \\lambda_k = 1$.\n",
    "\n",
    "In this notebook we will show an example using $K=2$ textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQJv1b_QMAOb"
   },
   "source": [
    "Here are some examples made with this notebook, using $K=2$ textures.\n",
    "\n",
    "![Text](https://raw.githubusercontent.com/storimaging/Images/main/SupportImages/TannatTulum.jpg)\n",
    "\n",
    "![Text](https://raw.githubusercontent.com/storimaging/Images/main/SupportImages/christmasdaisies.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h81V-9U4EwN0"
   },
   "source": [
    "## Importing packages\n",
    "\n",
    "Below is a list of packages needed to implement texture interpolation. PyTorch version used to run this notebook is **1.11.0+cu113** (to check the installed version, use `torch.__version__`)\n",
    "\n",
    "* `torch` (essential packages for neural networks with PyTorch)\n",
    "* `torch.optim` (efficient gradient descent)\n",
    "* `mse_loss` (to compute loss)\n",
    "* `torchvision.models` (to get the vgg network)\n",
    "* `torchvision.transforms.functional` (to transform images into tensors)\n",
    "* `PIL.Image, matplotlib.pyplot` (to load and display images)\n",
    "* `os` (to execute scripts with parameters into python code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sma2QpmZ4f6X"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import mse_loss\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import resize, to_tensor, normalize, to_pil_image\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77NiJnTW4f6X"
   },
   "source": [
    "## Loading images\n",
    "\n",
    "In the next section we will load images. Here we will just get, display and save the image, without doing any changes to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75XDTqWzwcru"
   },
   "outputs": [],
   "source": [
    "texture_imgnames = [\"arabicbread.jpg\", \"redfruits.jpg\", \"grapes.jpg\", \"tannat.jpg\", \"daisies.jpg\",\"tulum.jpg\",\"cloudsky.jpg\", \"monetwaterlily.jpg\", \"batlloexit.jpg\", \"christmas.jpg\", \"cheesecake.jpg\"]\n",
    "\n",
    "for fname in texture_imgnames:\n",
    "    os.system(\"wget -c https://raw.githubusercontent.com/storimaging/Images/main/Textures/\"+fname)\n",
    "    img = Image.open(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6LIdf1qBzZo"
   },
   "source": [
    "## Set a device\n",
    "\n",
    "Next, we need to choose which device to run the algorithm on. Running the algorithm on large images takes longer and will go much faster when running on a GPU. We can use `torch.cuda.is_available()` to detect if there is a GPU available. Next, we set the `torch.device`. The `.to(device)` method is used to move tensors or modules to a desired device, we will use it in next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGx_kjNJByZW"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\", device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01lplU6qAZaF"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "The original PIL images have values between 0 and 255, but when transformed into torch tensors, their values are converted to be between 0 and 1.\n",
    "\n",
    "An important detail to note is that neural networks from the torch library are trained with tensor values ranging from 0 to 1. Additionally, VGG networks are trained on images with each channel normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] (mean and standard deviation of Imagenet). We will need to normalize the image tensor before sending it into the network.\n",
    "\n",
    "Import the helper functions for loading, displaying and transforming into tensors by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0VwLA9x84Wb"
   },
   "outputs": [],
   "source": [
    "os.system(\"wget -nc https://raw.githubusercontent.com/storimaging/Notebooks/main/ImageGeneration/AuxiliarFunctions/textureInterpolation.py\")\n",
    "from textureInterpolation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jW7muFnJ9p9"
   },
   "source": [
    "## Model\n",
    "\n",
    "Now we need to import a pretrained neural network. We will use a 19-layer VGG network from PyTorch.\n",
    "\n",
    "VGG's PyTorch implementation is a module split into two sequential child modules: features (containing convolution and pooling layers) and classifier (containing fully connected layers). For the texture interpolation task, we are only interested in the layers of the features module. We won't let the parameters change: the network is already trained and used as an image transformation.\n",
    "\n",
    "On the output of the following commands, you can see the `feature` module structure. The indices will help to select the necessary layers for the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0k8pOOa6wPt"
   },
   "outputs": [],
   "source": [
    "cnn = models.vgg19(weights='IMAGENET1K_V1').features.to(device).eval()\n",
    "cnn.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jCB3teT1GEj"
   },
   "source": [
    "According to the algorithm explained at the beginning of this notebook, we need to access the outputs of some selected intermediate layers. In order to access the outputs of the layers on the PyTorch VGG19 network, we need to register a hook on each layer we need. Hooks are functions, which can be attached to each layer and called each time the layer is used. You can register a hook before or after the forward pass, or after the backward pass. We will define a function `save_output` that will be triggered after the forward pass, for each layer of the `features` module.\n",
    "\n",
    "The layer outputs will be stored in a dictionary where the key is the layer index and the value is the layer output tensor.\n",
    "\n",
    "We must define which layers will be part of the optimization. Using the indices of layers, we select the layers to use in the algorithm. We choose the outputs of the first five Conv2d layers, so indices are: 0, 2, 5, 7 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB2sDyreRY7m"
   },
   "outputs": [],
   "source": [
    "# Initialize outputs dic\n",
    "outputs = {}\n",
    "\n",
    "# Hook definition\n",
    "def save_output(name):\n",
    "\n",
    "    # The hook signature\n",
    "    def hook(module, module_in, module_out):\n",
    "        outputs[name] = module_out\n",
    "    return hook\n",
    "\n",
    "# Define layers\n",
    "layers = [0, 2, 5, 7, 10]\n",
    "\n",
    "# Register hook on each layer with index on array \"layers\"\n",
    "for layer in layers:\n",
    "    handle = cnn[layer].register_forward_hook(save_output(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a70keSW7YhMD"
   },
   "source": [
    "## Loss function\n",
    "\n",
    "We need to define the Loss function as explained at the beginning of the notebook. We first define some helper functions to calculate the mean, covariance and square root of a positive definite matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHSke_rY2GP4"
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def Mean_and_Cov(tensor):\n",
    "\n",
    "    a, b, c, d = tensor.size()\n",
    "    X = tensor.view(a*b, c*d)\n",
    "    M_tensor = X.mean(1)\n",
    "    C_tensor = torch.cov(X)\n",
    "    return M_tensor, C_tensor\n",
    "\n",
    "# Jonathan Vacher's code\n",
    "# https://github.com/JonathanVacher/texture-interpolation/blob/master/texture-synthesis-algorithm/models/utils.py\n",
    "def sqrtm(input):\n",
    "    \"\"\"Square root of a positive definite matrix.\n",
    "    NOTE: matrix square root is not differentiable for matrices with\n",
    "          zero eigenvalues.\n",
    "\n",
    "    See Lin, Tsung-Yu, and Subhransu Maji.\n",
    "        \"Improved Bilinear Pooling with CNNs.\" BMVC 17\n",
    "    \"\"\"\n",
    "    dim = input.shape[0]\n",
    "    norm = torch.norm(input.double())\n",
    "    Y = input/norm\n",
    "    I = torch.eye(dim,dim,device=input.device).type(input.dtype)\n",
    "    Z = torch.eye(dim,dim,device=input.device).type(input.dtype)\n",
    "    for i in range(15):\n",
    "        T = 0.5*(3.0*I - Z.mm(Y))\n",
    "        Y = Y.mm(T)\n",
    "        Z = T.mm(Z)\n",
    "    sqrtm = Y*torch.sqrt(norm)\n",
    "\n",
    "    return sqrtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9faSZjYNXlt"
   },
   "source": [
    "We now define a function to calculate the Wasser Loss. We use the following: $ B(Cov(x_l), Cov(u_l))^2 = \\left\\| Cov(x_l)^{1/2} - Cov(u_l)^{1/2} \\right\\|^2_F $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHKjVMHpYgkd"
   },
   "outputs": [],
   "source": [
    "def WasserLoss(input, M_target, SqrtCov_target):\n",
    "\n",
    "    M_x, C_x = Mean_and_Cov(input)\n",
    "    SqrtCov_tnsr = sqrtm(C_x)\n",
    "    return mse_loss(M_x, M_target, reduction='sum') + mse_loss(SqrtCov_tnsr, SqrtCov_target, reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRaeoQlLysy1"
   },
   "source": [
    "## Optimizer and initialization\n",
    "\n",
    "In this section we choose our textures to interpolate. These images are converted into tensors, then we compute the activations of the selected layers. Note that here we will interpolate between 2 textures, but we can easily modify the algorithm to interpolate between more than 2 images.\n",
    "\n",
    "For each activation of the target image, we compute the mean and the squareroot of the covariance matrix. This is done only once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX3N8IRSyz_f"
   },
   "outputs": [],
   "source": [
    "# Load target images\n",
    "# Select from the list: [\"arabicbread.jpg\", \"redfruits.jpg\", \"grapes.jpg\", \"tannat.jpg\", \"daisies.jpg\",\"tulum.jpg\",\"cloudsky.jpg\", \"monetwaterlily.jpg\", \"batlloexit.jpg\", \"christmas.jpg\", \"cheesecake.jpg\"]\n",
    "input_image_name1 = \"tannat.jpg\"\n",
    "input_image_name2 = \"tulum.jpg\"\n",
    "img_size = 256\n",
    "\n",
    "# Prepare texture data\n",
    "target1 = prep_img(input_image_name1, img_size).to(device)\n",
    "target2 = prep_img(input_image_name2, img_size).to(device)\n",
    "\n",
    "# Forward pass using targets textures for get activations of selected layers (outputs).\n",
    "cnn(target1)\n",
    "outputs_target1 = [outputs[key] for key in layers]\n",
    "cnn(target2)\n",
    "outputs_target2 = [outputs[key] for key in layers]\n",
    "\n",
    "# Get Mean, Cov and SQRT of Cov for each activation\n",
    "M_outputs_target1, C_outputs_target1, M_outputs_target2, C_outputs_target2, SqrtCov_target1, SqrtCov_target2 = list(), list(), list(), list(), list(), list()\n",
    "\n",
    "for i in range(len(layers)):\n",
    "    M_target1, C_target1 = Mean_and_Cov(outputs_target1[i])\n",
    "    M_target2, C_target2 = Mean_and_Cov(outputs_target2[i])\n",
    "    M_outputs_target1.append(M_target1)\n",
    "    C_outputs_target1.append(C_target1)\n",
    "    M_outputs_target2.append(M_target2)\n",
    "    C_outputs_target2.append(C_target2)\n",
    "    SqrtCov_target1.append(sqrtm(C_target1))\n",
    "    SqrtCov_target2.append(sqrtm(C_target2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDNRMAFvPGoM"
   },
   "source": [
    "\n",
    "Then, we draw the random initialization. This tensor needs to be optimized, so we set `requires_grad` to `True`.\n",
    "\n",
    "We use L-BFGS algorithm to run gradient descent. We will create a PyTorch L-BFGS optimizer `optim.LBFGS` and pass the `opt_img` image to it as the tensor to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_FmtQSBPFkQ"
   },
   "outputs": [],
   "source": [
    "# Random init for image opt_img\n",
    "opt_img = torch.randn_like(target1)\n",
    "opt_img.requires_grad=True\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = optim.LBFGS([opt_img], lr=0.1, max_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbJGXcY74f6Y"
   },
   "source": [
    "## Running texture interpolation\n",
    "\n",
    "We are now able to perform texture Interpolation.\n",
    "\n",
    "At each iteration of the network, it receives an updated input and computes new losses between target activations and opt activations (activations of the layers selected for the image that it is being optimized).\n",
    "\n",
    "As we are interpolating 2 textures we just have $\\lambda_1$ and $\\lambda_2$ as weights of each texture and $\\lambda_2 = 1 - \\lambda_1$.\n",
    "\n",
    "We will run the backward methods of each loss module to dynamically compute their gradients. The optimizer requires a “closure” function, which reevaluates the module and returns the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isjH-nC94f6Z"
   },
   "outputs": [],
   "source": [
    "# Inspired on Taras Savchyn's code\n",
    "# https://github.com/trsvchn/deep-textures/blob/main/example.ipynb\n",
    "\n",
    "iter_ = 0\n",
    "\n",
    "# Main loop of texture interpolation\n",
    "def textureInterpolation(lambda_1, n_iters, log_every, printresults):\n",
    "\n",
    "    global iter_\n",
    "    iter_ = 0\n",
    "\n",
    "    while iter_ <= n_iters:\n",
    "\n",
    "        def closure():\n",
    "            global iter_\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass using opt_img. Get activations of selected layers for image opt_img (outputs).\n",
    "            cnn(opt_img)\n",
    "            opt_outputs = [outputs[key] for key in layers]\n",
    "\n",
    "            # Compute loss\n",
    "            tex_score_1, tex_score_2 = 0, 0\n",
    "\n",
    "            for i in range(len(opt_outputs)):\n",
    "                tex_score_1 += WasserLoss(opt_outputs[i], M_outputs_target1[i], SqrtCov_target1[i])\n",
    "                tex_score_2 += WasserLoss(opt_outputs[i], M_outputs_target2[i], SqrtCov_target2[i])\n",
    "\n",
    "            loss = lambda_1*tex_score_1 + (1-lambda_1)*tex_score_2\n",
    "            loss.backward()\n",
    "\n",
    "            # Display results: print Loss value and show images\n",
    "            if (iter_ % log_every == 0 and printresults):\n",
    "                printResults(target1, target2, opt_img, iter_, loss, lambda_1)\n",
    "\n",
    "            iter_ += 1\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return opt_img\n",
    "\n",
    "\n",
    "opt_img = textureInterpolation(lambda_1 = 0.5, n_iters = 1000, log_every = 500, printresults=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov6VrdN_W05b"
   },
   "source": [
    "## Performing texture interpolation by moving barycenters\n",
    "\n",
    "In this section, we sample the path interpolating between two textures, that is, we compute the interpolating texture for different values ​​of  $\\lambda_1$ starting from a common random initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiJ8wf0OXBUN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def TextureInterpolationBarycenters (n_iters = 1000, n_Interpolations=10):\n",
    "\n",
    "    # Initialize list of images\n",
    "    image_list = []\n",
    "    init = torch.randn_like(target1)\n",
    "\n",
    "    for i in range(n_Interpolations):\n",
    "        print(\"Computing interpolation\", i+1, \"/\", n_Interpolations)\n",
    "\n",
    "        # Random init for image opt_img\n",
    "        opt_img = init.detach()\n",
    "        opt_img.requires_grad=True\n",
    "\n",
    "        # Set optimizer\n",
    "        optimizer = optim.LBFGS([opt_img], lr=0.1, max_iter=20)\n",
    "\n",
    "        # run interpolation\n",
    "        opt_img_final = textureInterpolation(lambda_1 = i/(n_Interpolations-1), n_iters = n_iters, log_every=  n_iters//10 , printresults=False)\n",
    "\n",
    "        # Store for comparison\n",
    "        image_list.append(opt_img_final.clone().detach().to(\"cpu\"))\n",
    "\n",
    "    return image_list\n",
    "\n",
    "image_list = TextureInterpolationBarycenters (n_iters = 1000, n_Interpolations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAiaUaJmvUOz"
   },
   "source": [
    "By running the following cell, we can compare the different interpolations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JAc4AHgb8q2"
   },
   "outputs": [],
   "source": [
    "from google.colab import widgets\n",
    "\n",
    "def compare_images(imgs):\n",
    "    labels = ['image ' + str(i) for i in range(len(imgs))]\n",
    "    tb = widgets.TabBar(labels, location='top')\n",
    "    for i, img in enumerate(imgs):\n",
    "        with tb.output_to(i, select=(i == 0)):\n",
    "            display(to_pil(img.squeeze(0)))\n",
    "\n",
    "compare_images(image_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
