{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/generativemodelingmva/generativemodelingmva.github.io/blob/main/tp2324/tp6_mva_diffusion_model_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion models: Sampling and conditional sampling of a pretrained DDPM model\n",
        "\n",
        "This notebook is based on the [github repository](https://github.com/DPS2022/diffusion-posterior-sampling.git) of\n",
        "\n",
        "*Diffusion Posterior Sampling for General Noisy Inverse Problems*,\n",
        "by Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, Jong Chul Ye,\n",
        "ICLR 2023, https://arxiv.org/abs/2209.14687\n",
        "\n",
        "We use the U-net trained by the authors on the FFHQ datasets (using 49k images, ```01000.png``` to ```49999.png```).\n",
        "\n",
        "It is a DDPM model based on\n",
        "\n",
        "* Diffusion Models Beat GANs on Image Synthesis, Prafulla Dhariwal, Alex Nichol, NeurIPS 2021, https://arxiv.org/abs/2105.05233\n",
        "[github](https://github.com/openai/guided-diffusion/)\n",
        "\n",
        "* Denoising Diffusion Probabilistic Models, Jonathan Ho, Ajay Jain, Pieter Abbeel, NeurIPS 2020, https://arxiv.org/abs/2006.11239\n",
        "[projectpage](https://hojonathanho.github.io/diffusion/)\n",
        "\n",
        "**Notebook author:**\n",
        "* Bruno Galerne: www.idpoisson.fr/galerne / https://github.com/bgalerne\n",
        "\n",
        "\n",
        "$\\newcommand{\\bx}{\\mathbf{x}} % bold x$\n",
        "$\\newcommand{\\bz}{\\mathbf{z}} % bold z$\n"
      ],
      "metadata": {
        "id": "U3wSq73fLtQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download files:"
      ],
      "metadata": {
        "id": "suFs9kiEMIYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DPS2022/diffusion-posterior-sampling.git\n",
        "!cp -r diffusion-posterior-sampling/guided_diffusion guided_diffusion\n",
        "!wget -nc -O ffhq256-1k-validation.zip 'https://www.dropbox.com/scl/fi/pppstbdsf0em6o0qscruc/ffhq256-1k-validation.zip?rlkey=xl7nwv2nxb6yvsirr3wad77hm'\n",
        "!unzip -nq ffhq256-1k-validation.zip\n",
        "!wget -nc -O ffhq_10m.pt 'https://www.dropbox.com/scl/fi/pq72vxzxcbygieq5z4gvf/ffhq_10m.pt?rlkey=5sxdj6r4o9f7b7bbp5fxg2f5r'\n",
        "\n"
      ],
      "metadata": {
        "id": "7HlRnEGgMHSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s3JciciLmvE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from guided_diffusion.unet import create_model\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display functions\n",
        "We will work with PyTorch images with color values in $[-1,1]$ and the usual additional batch dimension.\n",
        "Images will have size 1x3x256x256 in PyTorch."
      ],
      "metadata": {
        "id": "NF1v4oU7PQTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pilimg_to_tensor(pil_img):\n",
        "  t = torchvision.transforms.ToTensor()(pil_img)\n",
        "  t = 2*t-1 # [0,1]->[-1,1]\n",
        "  t = t.unsqueeze(0)\n",
        "  t = t.to(device)\n",
        "  return(t)\n",
        "\n",
        "def display_as_pilimg(t):\n",
        "  t = 0.5+0.5*t.to('cpu')\n",
        "  t = t.squeeze()\n",
        "  t = t.clamp(0.,1.)\n",
        "  pil_img = torchvision.transforms.ToPILImage()(t)\n",
        "  display(pil_img)\n",
        "  return(pil_img)\n",
        "\n",
        "idx = np.random.randint(1000)\n",
        "print('Image', str(idx).zfill(5))\n",
        "img_pil = Image.open('ffhq256-1k-validation/'+str(idx).zfill(5)+'.png')\n",
        "display(img_pil)\n",
        "display_as_pilimg(pilimg_to_tensor(img_pil));\n"
      ],
      "metadata": {
        "id": "YxHHc0A4Lskr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load DDPM Unet"
      ],
      "metadata": {
        "id": "WDDJMYOQbMV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model_config = {'image_size': 256,\n",
        "                'num_channels': 128,\n",
        "                'num_res_blocks': 1,\n",
        "                'channel_mult': '',\n",
        "                'learn_sigma': True,\n",
        "                'class_cond': False,\n",
        "                'use_checkpoint': False,\n",
        "                'attention_resolutions': 16,\n",
        "                'num_heads': 4,\n",
        "                'num_head_channels': 64,\n",
        "                'num_heads_upsample': -1,\n",
        "                'use_scale_shift_norm': True,\n",
        "                'dropout': 0.0,\n",
        "                'resblock_updown': True,\n",
        "                'use_fp16': False,\n",
        "                'use_new_attention_order': False,\n",
        "                'model_path': 'ffhq_10m.pt'}\n",
        "model = create_model(**model_config)\n",
        "model = model.to(device)\n",
        "# use in eval mode:\n",
        "model.eval();\n"
      ],
      "metadata": {
        "id": "sd1A0WSxP_JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDPM class\n",
        "\n",
        "The above DDPM class will be used to sample the DDPM model.\n",
        "\n",
        "The Unet ```model``` estimates the residual noise $\\varepsilon_t(\\bx_t, t)$ from a noisy image $(\\bx_t, t)$.\n",
        "\n",
        "The method ```get_eps_from_model(self, x, t)``` computes\n",
        "\n",
        "$$\n",
        "\\hat{\\bx_0}(\\bx_t) = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\bx_t - \\sqrt{\\frac{1}{\\bar{\\alpha}_t}-1} \\varepsilon_t(\\bx_t, t).\n",
        "$$"
      ],
      "metadata": {
        "id": "90ZFfhQObVd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPM:\n",
        "  def __init__(self, model=model):\n",
        "    self.num_diffusion_timesteps = 1000\n",
        "    self.reversed_time_steps = np.arange(self.num_diffusion_timesteps)[::-1]\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    self.betas = np.linspace(beta_start, beta_end, self.num_diffusion_timesteps,\n",
        "                              dtype=np.float64)\n",
        "    self.alphas = 1.0 - self.betas\n",
        "    self.alphas_cumprod = np.cumprod(self.alphas, axis=0)\n",
        "    self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
        "    self.model = model\n",
        "    self.imgshape = (1,3,256,256)\n",
        "\n",
        "\n",
        "  def get_eps_from_model(self, x, t):\n",
        "    # the model outputs:\n",
        "    # - an estimation of the noise eps (chanels 0 to 2)\n",
        "    # - learnt variances for the posterior  (chanels 3 to 5)\n",
        "    # (see Improved Denoising Diffusion Probabilistic Models\n",
        "    # by Alex Nichol, Prafulla Dhariwal\n",
        "    # for the parameterization)\n",
        "    # We discard the second part of the output for this practice session.\n",
        "    model_output = self.model(x, torch.tensor(t, device=device).unsqueeze(0))\n",
        "    model_output = model_output[:,:3,:,:]\n",
        "    return(model_output)\n",
        "\n",
        "  def predict_xstart_from_eps(self, x, eps, t):\n",
        "    x_start = (\n",
        "        np.sqrt(1.0 / self.alphas_cumprod[t])* x\n",
        "        - np.sqrt(1.0 / self.alphas_cumprod[t] - 1) * eps\n",
        "    )\n",
        "    x_start = x_start.clamp(-1.,1.)\n",
        "    return(x_start)\n",
        "\n",
        "  def sample(self, show_steps=True):\n",
        "    #TODO\n",
        "    # init xT\n",
        "    with torch.no_grad():\n",
        "      x = torch.randn(self.imgshape,device=device)\n",
        "\n",
        "      #TODO\n",
        "\n",
        "    return(x)\n",
        "\n",
        "  def posterior_sampling(self, linear_operator, y, x_true=None, show_steps=True, vis_y=None):\n",
        "\n",
        "    # visualization image for y:\n",
        "    if vis_y==None:\n",
        "      vis_y = y\n",
        "\n",
        "    # init xT\n",
        "    x = torch.randn(self.imgshape,device=device)\n",
        "    x.requires_grad = True\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    return(x)\n",
        "\n",
        "\n",
        "ddpm = DDPM()\n"
      ],
      "metadata": {
        "id": "iBq6LVrRSh2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Denoiser visualization\n",
        "1. Take an image from FFHQ validation set and apply the forward model to it:\n",
        "$$\n",
        "\\bx_{t+1} = \\sqrt{\\alpha_t} \\bx_t + \\sqrt{\\beta_t} \\bz_t\n",
        "$$\n",
        "Display $\\bx_t$ and $\\hat{\\bx_0}(\\bx_t)$ for some 10 different levels.\n",
        "\n",
        "2. Display the two curves of the PSNR$(\\bx_t,\\bx_0)$ and PSNR$(\\hat{\\bx_0}(\\bx_t),\\bx_0)$.\n"
      ],
      "metadata": {
        "id": "hUNKWONCWCnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 1:\n",
        "# Image 00462\n",
        "idx = 462\n",
        "img_pil = Image.open('ffhq256-1k-validation/'+str(idx).zfill(5)+'.png')\n",
        "x0 = pilimg_to_tensor(img_pil)\n",
        "xt = x0.clone()\n",
        "psnr_noisy = []\n",
        "psnr_denoised = []\n",
        "def mypsnr(x,y):\n",
        "  error = torch.mean((x-y)**2).item()\n",
        "  psnr = 10*np.log10(2**2/error)\n",
        "  return(psnr)\n",
        "\n",
        "# TODO below\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SiWV24nOXR1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Q2 plot\n"
      ],
      "metadata": {
        "id": "ck6Lja1_-lec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Sampling\n",
        "1. Complete the DDPM class with a sample function. Let us recall that the DDPM transition probability is given by\n",
        "$$\n",
        "p_\\theta(\\bx_{t-1}|\\bx_{t})\n",
        "= \\mathcal{N}(\\mu_\\theta(\\bx_{t}, t), \\beta_t I_d)\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\mu_\\theta(\\bx_{t}, t)\n",
        "=\n",
        "\\frac{\\beta_t\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t}} \\hat{\\bx_0}(\\bx_t,t)\n",
        "+\n",
        "\\frac{(1-\\bar{\\alpha}_{t-1})\\sqrt{\\alpha_t}}{1-\\bar{\\alpha}_{t}} \\bx_t.\n",
        "$$\n",
        "\n",
        "2. Add an option to the method ```sample``` to display both $\\bx_t$ and $\\hat{\\bx_0}(\\bx_t,t)$ every 100 iterations. Observe the evolution of the samples.\n"
      ],
      "metadata": {
        "id": "MTsw-SyUi5h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = ddpm.sample()"
      ],
      "metadata": {
        "id": "XaHxo0W_i4Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional sampling for imaging inverse problems\n",
        "\n",
        "We will perform conditional sampling for linear inverse problems based on\n",
        "\n",
        "*Diffusion Posterior Sampling for General Noisy Inverse Problems*,\n",
        "by Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, Jong Chul Ye,\n",
        "ICLR 2023, https://arxiv.org/abs/2209.14687\n",
        "\n",
        "We restrict to linear measurements with Gaussian noise (eg inpainting, super resolution, deblurring,...).\n",
        "\n",
        "The algorithm is the following:\n",
        "Given\n",
        "$\\mathbf{y} = A \\bx + \\eta$ where $A$ is a linear operator and $\\eta$ is some Gaussian additive noise, we want to approximately sample\n",
        "$$\n",
        "p_\\theta(\\bx_0| \\mathbf{y} = A \\bx_0 + \\eta)\n",
        "$$\n",
        "This is performed by adding a correction term to the sampling procedure:\n",
        "\n",
        "\n",
        "> Initialize $x_T$ as for unconditional sampling.\n",
        ">\n",
        "> For $t=T$ to $1$:\n",
        "  1. Predict $\\hat{\\bx_0}(\\bx_t,t)$.\n",
        "  2. Compute the $\\ell^2$ error $\\|A\\hat{\\bx_0}(\\bx_t,t) - \\mathbf{y} \\|^2$.\n",
        "  3. Define\n",
        "  $$\n",
        "  \\bx_{t-1} =\n",
        "  \\mu_\\theta(\\bx_{t}, t)\n",
        "  + \\sqrt{\\beta_t} \\bz\n",
        "  - \\zeta_t \\nabla_{\\bx_t} \\|A\\hat{\\bx_0}(\\bx_t,t) - \\mathbf{y} \\|^2.\n",
        "  $$\n",
        "where the scaling factor $\\zeta_t$ has been experimentally fixed as $\\|A\\hat{\\bx_0}(\\bx_t,t) - \\mathbf{y} \\|^{-1}$.\n",
        "\n",
        "Note that computing $\\nabla_{\\bx_t} \\|A\\hat{\\bx_0}(\\bx_t,t) - \\mathbf{y} \\|^2$ involves a backpropagation through the Unet so one can expect the conditional sampling to be twice as long as the sampling procedure.\n",
        "\n",
        "# Exercise 3: Conditional sampling\n",
        "1. Add a method ```posterior_sampling```to the DDPM class\n",
        "> posterior_sampling(self, linear_operator, y, x_true=None, show_steps=True, viz_y = None)\n",
        "\n",
        "that allows to display every 100 iterations the images $\\bx_t$ and $\\hat{\\bx_0}(\\bx_t,t)$ as well as the image $\\mathbf{y}$ (we suppose that there is a natural way to visualize $\\mathbf{y}$ as an image having the same size as $\\mathbf{x}_t$, defined by the tensor ```vis_y``` if $\\mathbf{y}$ does not have the same size, eg masked image for inpainting, nearest neighbor interpolation for super resolution...) and the groundtruth image if given.\n",
        "2. Test it with the inpainting example below.\n",
        "3. Define the operator to solve a x4 super-resolution problem and test it on some portrait image.\n",
        "\n"
      ],
      "metadata": {
        "id": "Et6ZYFPVHSJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = 256\n",
        "w = 256\n",
        "hcrop, wcrop = h//2, w//2\n",
        "corner_top, corner_left = h//4, int(0.45*w)\n",
        "mask = torch.ones(ddpm.imgshape, device=device)\n",
        "mask[:,:,corner_top:corner_top+hcrop,corner_left:corner_left+wcrop] = 0\n",
        "\n",
        "def inpainting(x):\n",
        "  x = x*mask\n",
        "  return(x)\n",
        "\n",
        "linear_operator = inpainting\n",
        "\n",
        "idx = 12\n",
        "x_true_pil = Image.open('ffhq256-1k-validation/'+str(idx).zfill(5)+'.png')\n",
        "x_true = pilimg_to_tensor(x_true_pil)\n",
        "print(x_true.device)\n",
        "print(\"original image\", str(idx).zfill(5)+'.png')\n",
        "display_as_pilimg(x_true)\n",
        "\n",
        "sigma_noise = 2*10/255\n",
        "\n",
        "y = linear_operator(x_true.clone()) + sigma_noise * mask * torch.randn_like(x_true)\n",
        "print(\"noisy measurement\")\n",
        "display_as_pilimg(y);\n"
      ],
      "metadata": {
        "id": "EaQqH-WdR9Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddpm.posterior_sampling(linear_operator, y, x_true=x_true, show_steps=True)"
      ],
      "metadata": {
        "id": "YCzfALMZSPgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p20qS5wVS1YS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}