{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3aqjgrs4f6U"
   },
   "source": [
    "# Texture synthesis with CNNs in PyTorch\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/storimaging/Notebooks/blob/main/ImageGeneration/CNN_Texture_Synthesis_with_solution.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsZ7qPBk4tO0"
   },
   "source": [
    "## Introduction ##\n",
    "\n",
    "This practical session explains how to implement the Texture Synthesis based on the algorithm described on [L. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems, pages 262–270, 2015. 4](https://arxiv.org/abs/1505.07376).\n",
    "\n",
    "**Texture Synthesis:**\n",
    "\n",
    "Given an input texture image, produce an output texture image that is both visually similar and pixel-wise different from the input texture. The output image should ideally be perceived as another part of the same large piece of homogeneous material from which the input texture originated.\n",
    "\n",
    "**References:**\n",
    "\n",
    "This practical session is based on several resources:\n",
    "\n",
    "*   Original code: https://github.com/leongatys/DeepTextures\n",
    "*   Reimplementation: https://github.com/trsvchn/deep-textures\n",
    "*   Tutorial used for some explanation: https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "\n",
    "**Authors:**\n",
    "* [Bruno Galerne](https://github.com/bgalerne)\n",
    "* Lucía Bouza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRYqPQm-_rpY"
   },
   "source": [
    "## Underlying principle\n",
    "\n",
    "Let us recall the algorithm proposed by Gatys et al.\n",
    "Given an example image $u$ and a random initialization $x_0$,\n",
    "we optimize the loss function\n",
    "$$\n",
    "E(x) = \\sum_{\\text{for selected layers } L} w_L\\left\\| G^L(x) - G^L(u) \\right\\|^2_F\n",
    "$$\n",
    "where $\\|\\cdot\\|_F$ is the Frobenius norm and for an image $y$ and a layer index $L$ $G^L(y)$ denotes the Gram matrix of the VGG-19 features at layer $L$:\n",
    "if $V^L(y)$ is the feature response of $y$ at layer $L$ that has spatial size $w\\times h$ and $n$ channels,\n",
    "$$\n",
    "G^L(y) = \\frac{1}{w h}\\sum_{k\\in \\{0,\\dots,w-1\\}\\times\\{0,\\dots,h-1\\}} V^L(y)_k V^L(y)_k^T \\in \\mathbb{R}^{n\\times n}.\n",
    "$$\n",
    "The optimization is done using the L-BFGS algorithm.\n",
    "\n",
    "## Exercise:\n",
    "\n",
    "\n",
    "1. Go through the code and execute the algorithm.\n",
    "\n",
    "2. We use the outputs of 5 VGG-19 layers to define $E$. Verify that the quality of the output texture decreases if fewer layers are used (e.g. only the first layer or the three first layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h81V-9U4EwN0"
   },
   "source": [
    "## Importing packages\n",
    "\n",
    "Below is a list of packages needed to implement texture synthesis. PyTorch version used to run this notebook is **1.11.0+cu113** (to check the installed version, use `torch.__version__`)\n",
    "\n",
    "* `torch` (indispensable packages for neural networks with PyTorch)\n",
    "* `torch.optim` (efficient gradient descent)\n",
    "* `mse_loss` (compute loss)\n",
    "* `torchvision.models` (get the vgg network)\n",
    "* `torchvision.transforms.functional` (transform images into tensors)\n",
    "* `PIL.Image, matplotlib.pyplot` (display of images and graphics)\n",
    "* `os` (interactions with the operating system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sma2QpmZ4f6X"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import mse_loss\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import resize, to_tensor, to_pil_image\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZXWSV-k9Frr"
   },
   "source": [
    "To import the solutions, execute the following cell. If you are using a Windows system, comment the `os.system` line, download the file by hand, and place it in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_gnEAVl8-xC"
   },
   "outputs": [],
   "source": [
    "#os.system(\"wget -nc https://raw.githubusercontent.com/storimaging/Notebooks/main/ImageGeneration/Solutions/TS.py\")\n",
    "#from TS import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77NiJnTW4f6X"
   },
   "source": [
    "## Loading images\n",
    "\n",
    "In the next section we will load images. Here we'll just get, display, and save the image, without making any changes to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75XDTqWzwcru"
   },
   "outputs": [],
   "source": [
    "texture_imgnames = [\"bois.png\", \"briques.png\", \"mur.png\", \"tissu.png\", \"nuages.png\",\"pebbles.jpg\",\"wall1003.png\"]\n",
    "\n",
    "for fname in texture_imgnames:\n",
    "    os.system(\"wget -c https://raw.githubusercontent.com/storimaging/Images/main/Textures/\"+fname)\n",
    "    img = Image.open(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6LIdf1qBzZo"
   },
   "source": [
    "## Set a device\n",
    "\n",
    "Next, we need to choose which device to run the algorithm on. Running the algorithm on large images takes longer and will go much faster when running on a GPU. We can use `torch.cuda.is_available()` to detect if there is a GPU available. Next, we set the `torch.device`. The `.to(device)` method is used to move tensors or modules to a desired device, we will use it in next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGx_kjNJByZW"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\", device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01lplU6qAZaF"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "An important detail to note is that the neural network model we will use is trained on preprocessing images. We will need to apply the same preprocessing step to the image tensor before sending it into the network.\n",
    "\n",
    "The preprocessing step consists only on subtracting the mean RGB value, computed on the training set (Imagenet), from each pixel. The original PIL images have values between 0 and 255, but when transformed into torch tensors, their values are converted to be between 0 and 1. Therefore, after subtracting the mean (`[0.485, 0.456, 0.406]`), it is necessary to multiply the tensor by 255 to the values be between 0 and 255 again.\n",
    "Read the following [paper](https://arxiv.org/pdf/1409.1556.pdf) on section 2.1 to learn more details about VGG training.\n",
    "\n",
    "Import the helper functions for loading and displaying images by running the cell below. If you have imported the solutions, you don't need to run this cell, since this functions are arelady included in the Solution's file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBHpwxZwEegw"
   },
   "outputs": [],
   "source": [
    "os.system(\"wget -nc https://raw.githubusercontent.com/storimaging/Notebooks/main/ImageGeneration/AuxiliarFunctions/textureSynthesis.py\")\n",
    "from textureSynthesis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jW7muFnJ9p9"
   },
   "source": [
    "## Model\n",
    "\n",
    "Now we need to define the neural network and import a pretrained model. We will use a 19-layer VGG network from PyTorch and the pretrained model used on the [paper](https://arxiv.org/abs/1505.07376). We use this particular model because it is a normalized version of the VGG network. Here, the weights are scaled such that the mean activation of each filter over training images and positions is equal to 1. The benefit of normalization is that losses based on features extracted from different layers of the network will have comparable magnitude.\n",
    "On this [github site](https://github.com/corleypc/vgg-normalize) you can find a detailed explanation of how  normalization works and an implementation to do it. Also [this thread](https://stats.stackexchange.com/questions/361723/weight-normalization-technique-used-in-image-style-transfer) can help to understand the process.  \n",
    "\n",
    "VGG's PyTorch implementation is a module split into two sequential child modules: features (containing convolution and pooling layers) and classifier (containing fully connected layers). For the texture synthesis task, we are only interested in the layers of the features module. We won't let the parameters change: the network is already trained and used as an image transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYdf_ritNGRS"
   },
   "source": [
    "First we will need to download the pretrained model. This model is stored on Google Drive, but is the same metioned on the paper (The [official site](http://bethgelab.org/media/uploads/deeptextures/vgg_normalised.caffemodel) sometimes is down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLksnkGFF-0S"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download(f\"https://drive.google.com/uc?export=download&confirm=pbef&id=1lLSi8BXd_9EtudRbIwxvmTQ3Ms-Qh6C8\",'vgg_conv.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80Lb-DiaNDjO"
   },
   "source": [
    "We cannot directly load the model into the PyTorch VGG network because the keys are not the same in the pretrained model. That's why we iterate over the VGG parameters and the pretrained model dictionary, to load the weights.\n",
    "\n",
    "In the output of the following commands, you can see the `feature` module structure. The indexes will help to select the necessary layers for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te32Cde1MWaj"
   },
   "outputs": [],
   "source": [
    "cnn = models.vgg19(weights=None).features.to(device)\n",
    "pretrained_dict = torch.load('vgg_conv.pth')\n",
    "\n",
    "for param, item in zip(cnn.parameters(), pretrained_dict.keys()):\n",
    "    param.data = pretrained_dict[item].type(torch.FloatTensor).to(device)\n",
    "\n",
    "cnn.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jCB3teT1GEj"
   },
   "source": [
    "According to the algorithm explained at the beginning of this notebook, we need to access the outputs of some selected intermediate layers. In order to access the outputs of the layers on the PyTorch VGG19 network, we need to register a hook on each layer we need. Hooks are functions, which can be attached to each layer and called each time the layer is used. You can register a hook before or after the forward pass, or after the backward pass. We will define a function `save_output` that will be triggered after the forward pass, for each layer of the `features` module.\n",
    "\n",
    "The layer outputs will be stored in a dictionary where the key is the layer index and the value is the layer output tensor.\n",
    "\n",
    "So we need to define which layers will be part of the optimization and define weights for each one (we will use the weights when performing the texture synthesis). Using the layer indices, we select the layers to use in the algorithm. The output of the first conv layer and the outputs of the first conv layer following each pools layer are a good selection. This is why we choose the indices 1, 6, 11, 20, 29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB2sDyreRY7m"
   },
   "outputs": [],
   "source": [
    "# Initialize outputs dic\n",
    "outputs = {}\n",
    "\n",
    "# Hook definition\n",
    "def save_output(name):\n",
    "\n",
    "    # The hook signature\n",
    "    def hook(module, module_in, module_out):\n",
    "        outputs[name] = module_out\n",
    "    return hook\n",
    "\n",
    "# Define layers\n",
    "layers = [1, 6, 11, 20, 29]\n",
    "# Define weights for layers\n",
    "layers_weights = [1/n**2 for n in [64,128,256,512,512]]\n",
    "\n",
    "# Register hook on each layer with index on array \"layers\"\n",
    "for layer in layers:\n",
    "    handle = cnn[layer].register_forward_hook(save_output(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a70keSW7YhMD"
   },
   "source": [
    "## Loss function\n",
    "\n",
    "We need to define the Loss function as explained at the beginning of the notebook. We first define a function to calculate the Gram Matrix, then a loss function that computes the MSE for 2 Gram matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHKjVMHpYgkd"
   },
   "outputs": [],
   "source": [
    "# Computes Gram matrix for the input batch tensor.\n",
    "#    Args: tnsr (torch.Tensor): input tensor of the Size([B, C, H, W]).\n",
    "#    Returns:  G (torch.Tensor): output tensor of the Size([B, C, C]).\n",
    "def gramm(tnsr: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    b,c,h,w = tnsr.size()\n",
    "    F = tnsr.view(b, c, h*w)\n",
    "    G = torch.bmm(F, F.transpose(1,2))\n",
    "    G.div_(h*w)\n",
    "    return G\n",
    "\n",
    "# Computes MSE Loss for 2 Gram matrices\n",
    "def gram_loss(input: torch.Tensor, gramm_target: torch.Tensor, weight: float = 1.0):\n",
    "\n",
    "    loss = weight * mse_loss(gramm(input), gramm_target)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRaeoQlLysy1"
   },
   "source": [
    "## Optimizer and initialization\n",
    "\n",
    "In this section we choose the target texture. This image is converted into a tensor, then the activations of the layers selected for this tensor are computed (dictionary `outputs` after forward pass using target texture). We  will also compute Gram Matrix for these activations (these values doesn't change so is efficient calculate them only once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX3N8IRSyz_f"
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "### This section allows you to change target image ###\n",
    "######################################################\n",
    "# Select input image: [\"bois.png\", \"briques.png\", \"mur.png\", \"tissu.png\", \"nuages.png\",\"pebbles.jpg\",\"wall1003.png\"]\n",
    "input_image_name = \"wall1003.png\"\n",
    "img_size = 512\n",
    "\n",
    "# Prepare texture data\n",
    "target = prep_img(input_image_name, img_size).to(device)\n",
    "target_img = to_pil(target)\n",
    "plt.axis('off')\n",
    "plt.imshow(target_img)\n",
    "\n",
    "# Forward pass using target texture for get activations of selected layers (outputs). Calculate gram Matrix for those activations\n",
    "cnn(target)\n",
    "gramm_targets = [gramm(outputs[key]) for key in layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RkWFUgeYSvp"
   },
   "source": [
    "Then, we draw the random initialization. This tensor needs to be optimized, so we set `requires_grad` to `True`.\n",
    "\n",
    "We use L-BFGS algorithm to run gradient descent. We will create a PyTorch L-BFGS optimizer `optim.LBFGS` and pass the `synth` image to it as the tensor to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pfpHRZJYWUE"
   },
   "outputs": [],
   "source": [
    "# Random init for image synth\n",
    "synth = torch.randn_like(target)*10\n",
    "synth.requires_grad=True\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = optim.LBFGS([synth])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbJGXcY74f6Y"
   },
   "source": [
    "## Running texture synthesis\n",
    "\n",
    "We are now able to perform texture synthesis.\n",
    "\n",
    "At each iteration of the network, it receives an updated input and computes new losses between target activations and synth activations (activations of the layers selected for the image that it is being optimized).\n",
    "\n",
    "We will run the backward methods of each loss module to dynamically compute their gradients. The optimizer requires a “closure” function, which reevaluates the module and returns the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIgR15Da58N7"
   },
   "outputs": [],
   "source": [
    "def textureSynthesis (n_iters, log_every, synth, cnn, target, gramm_targets, outputs, layers, layers_weights, optimizer):\n",
    "    global iter_\n",
    "    iter_ = 0\n",
    "    print(iter_)\n",
    "\n",
    "    while iter_ <= n_iters:\n",
    "\n",
    "        def closure():\n",
    "            global iter_\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass using synth. Get activations of selected layers for image synth (outputs).\n",
    "            cnn(synth)\n",
    "            synth_outputs = [outputs[key] for key in layers]\n",
    "\n",
    "            # Compute loss for each activation\n",
    "            losses = []\n",
    "            for activations in zip(synth_outputs, gramm_targets, layers_weights):\n",
    "                losses.append(gram_loss(*activations).unsqueeze(0))\n",
    "\n",
    "            total_loss = torch.cat(losses).sum()\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Display results: print Loss value and show images\n",
    "            if iter_ % log_every == 0:\n",
    "                printResults(target, synth, iter_, total_loss)\n",
    "\n",
    "            iter_ += 1\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return synth\n",
    "\n",
    "n_iters = 1000\n",
    "log_every = 500\n",
    "\n",
    "synth = textureSynthesis (n_iters, log_every, synth, cnn, target, gramm_targets, outputs, layers, layers_weights, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uuFTrmr1U6W"
   },
   "source": [
    "# Exercise 2:\n",
    "\n",
    "Pick one of the following problems:\n",
    "\n",
    "* **A: Color correction**\n",
    "  \n",
    "  Observe that for some image the color is inconsistant (eg with `wall1003.png`). A solution to correct the output color distribution is to incorporate the mean color and the color covariance as a target statistics in $E$.\n",
    "\n",
    "  **Hint**: Considering mean color vector $m$ and covariance matices $C_h = C_h(0)$:\n",
    "\n",
    "  $$\n",
    "  m = \\begin{pmatrix}\n",
    "  m_r \\\\\n",
    "  m_g \\\\\n",
    "  m_b\n",
    "  \\end{pmatrix}\n",
    "  = \\frac{1}{MN}\\sum_{t\\in\\Omega} h(t) \\in \\mathbb{R}^{3}\n",
    "  $$\n",
    "  $$\n",
    "  C_h = \\frac{1}{MN}\\sum_{t\\in\\Omega}\n",
    "  \\begin{pmatrix}\n",
    "  h_r(t) - m_r \\\\\n",
    "  h_g(t) - m_g \\\\\n",
    "  h_b(t) - m_b\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "  h_r(t) - m_r \\\\\n",
    "  h_g(t) - m_g \\\\\n",
    "  h_b(t) - m_b\n",
    "  \\end{pmatrix}^T\n",
    "  \\in\\mathbb{R}^{3\\times 3}.\n",
    "  $$\n",
    "\n",
    "  Then change $E$ to:\n",
    "  $ E + \\lambda_{mean} \\| m(x) - m(u)\\|^2 + \\lambda_{cov} \\| C(x) - C(u)\\|^2. $\n",
    "\n",
    "  Try with $\\lambda_{mean}$ and $\\lambda_{cov}$ between 0.1 and 10.\n",
    "\n",
    "\n",
    "\n",
    "* **B: Spectral correction**\n",
    "\n",
    "  Add a term to the energy that would enforce a consistency with the original Fourier spectrum of each color channel, that is change $E$ to:\n",
    "$$\n",
    "E + \\lambda_{Fourier} \\| |\\hat{x}| - |\\hat{u}|\\|^2.\n",
    "$$\n",
    "Try with $\\lambda_{Fourier}$ between 1 and 0.01 to see the differences. Experiment with different types of textures.\n",
    "What is the interest of this approach?\n",
    "What are the textures for which it improves or degrades the quality of the result?\n",
    "\n",
    "\n",
    "\n",
    "* **C: Order one statistics**\n",
    "\n",
    "  Replace $E$ so that the spatial average of ALL the VGG-19 layers is preserved,. Change $E$ to:\n",
    "$$\n",
    "  E_{mean} (x) = \\sum_{\\text{for all layers } L} w_L \\left\\| \\operatorname{mean}(V^L(x)) - \\operatorname{mean}(V^L(u)) \\right\\|^2_F\n",
    "$$\n",
    "  Here, consider weighting the layers with the same approach made in the notebook. The mean is computed along the spatial dimension, so for each layer the mean vector has size \"number of channels within the layer\".\n",
    " Compare with the original model. What is the interest of this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZU6wP9CDVAHD"
   },
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OzQAftVVCFI"
   },
   "source": [
    "### Exercise A\n",
    "\n",
    "We will change the loss function. We will add to the Gramm loss the \"A_loss\". `A_loss` computes the MSE (weighted by lambda1) between means of colors of target and synth image, as well as the MSE (weighted by lambda2) of the covariances matrices of the target and synth image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CllKaEfVMBh"
   },
   "source": [
    "Run this cell to use another texture image and restart the synth image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NG4xmO2OVMpq"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "image = \"pebbles.jpg\"\n",
    "img_size = 256\n",
    "\n",
    "# Prepare texture data\n",
    "target = prep_img(image, img_size).to(device)\n",
    "\n",
    "# Forward pass using target texture for get activations of selected layers (outputs). Calculate gram Matrix for those activations\n",
    "cnn(target)\n",
    "gramm_targets = [gramm(outputs[key]) for key in layers]\n",
    "\n",
    "# Random init for image synth\n",
    "synth = torch.randn_like(target) * 10\n",
    "synth.requires_grad=True\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = optim.LBFGS([synth])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcXYlgmMc6jx"
   },
   "source": [
    "The next cell runs the new optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07FNaXQv7FOA"
   },
   "outputs": [],
   "source": [
    "n_iters = 2000\n",
    "log_every = 1000\n",
    "\n",
    "synth = textureSynthesisA (n_iters, log_every, synth, cnn, target, gramm_targets, outputs, layers, layers_weights, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2cjXvKPVj7L"
   },
   "source": [
    "### Exercise B\n",
    "\n",
    "We will change the loss function. We will add to the Gramm loss the \"B_loss\". `B_loss` computes the MSE (weighted by lambda3) between Fourier absolute values for the target and synth image. By adding this statistic to the loss function, we are taking into account periodic signals on images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoLyv7-xVqE2"
   },
   "source": [
    "Run this cell to use another texture image and restart the synth image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jFYCwlJEVu3j"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "image = \"briques.png\"\n",
    "img_size = 256\n",
    "\n",
    "# Prepare texture data\n",
    "target = prep_img(image, img_size).to(device)\n",
    "target_img = to_pil(target)\n",
    "\n",
    "# Forward pass using target texture for get activations of selected layers (outputs). Calculate gram Matrix for those activations\n",
    "cnn(target)\n",
    "gramm_targets = [gramm(outputs[key]) for key in layers]\n",
    "\n",
    "# Random init for image synth\n",
    "synth = torch.randn_like(target) * 10\n",
    "synth.requires_grad=True\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = optim.LBFGS([synth])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFYOTqI4WPjw"
   },
   "source": [
    "The next cell runs the new optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXjPLTpNcIQs"
   },
   "outputs": [],
   "source": [
    "n_iters = 3000\n",
    "log_every = 1000\n",
    "\n",
    "synth = textureSynthesisB (n_iters, log_every, synth, cnn, target, gramm_targets, outputs, layers, layers_weights, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvwSDMODWZDW"
   },
   "source": [
    "For textures with patterns, when lambda increases (example lambda = 1), the synthesis captures the patterns, but has difficulty capturing colors, for example. For smaller lambdas (example lambda = 0.01), colors are taken into account, but the model has a harder time capturing patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAYBQDKRWc3W"
   },
   "source": [
    "### Exercise C\n",
    "\n",
    "With this approach we don't need to calculate Gram Matrices, so amount of parameters decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWc-qMwqWh86"
   },
   "source": [
    "Run this cell to use another texture image and restart the synth image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "TlcK4AG-WlsB"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "image = \"pebbles.jpg\"\n",
    "img_size = 256\n",
    "\n",
    "# Prepare texture data\n",
    "target = prep_img(image, img_size).to(device)\n",
    "target_img = to_pil(target)\n",
    "\n",
    "# Random init for image synth\n",
    "synth = torch.randn_like(target) * 10\n",
    "synth.requires_grad=True\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = optim.LBFGS([synth])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhXl--ELWpzr"
   },
   "source": [
    "We will need to register the hook on all layers because we need the outputs of all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdYRf5KSWpA_"
   },
   "outputs": [],
   "source": [
    "# Register hook on each layer with index on array \"layers\"\n",
    "for layer in range(37):\n",
    "    handle = cnn[layer].register_forward_hook(save_output(layer))\n",
    "\n",
    "#Get target activations of all layers\n",
    "cnn(target)\n",
    "target_outputs = [outputs[key] for key in range(37)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pR7MM7u8W7XA"
   },
   "source": [
    "The next cell runs the new optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg0Uel2jcb_d"
   },
   "outputs": [],
   "source": [
    "n_iters = 3000\n",
    "log_every = 1000\n",
    "\n",
    "synth = textureSynthesisC (n_iters, log_every, synth, cnn, target, target_outputs, outputs, optimizer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
